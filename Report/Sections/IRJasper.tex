\subsection{Language detection \& spell check}
\textbf{Name:} Jasper Selman \indent \textbf{StudentNumber:} 0741516

\subsubsection*{Motivation}
When retrieving data from the internet there is a problem which every programmer faces: the incapability of the human being to agree on one single set of unambiguous rules to write words and the ability to actually write according to these rules. On most sites there are a lot of spelling errors made and words are written in a different way because the grammatical and spelling rules let him do that. \\
When you want to index the data retrieved you do not want that two words who only differ in some spelling error are indexed different from each other. That's why we need a spell check. We also do not want the same words in different languages to differ from each other. When we focus on local news sites this will not be a problem, but if we use Twitter data this might be a problem.

\subsubsection*{Approach}
At the start we had to discuss what we really wanted to enhance, since there are a lot of possibilities to improve and or check texts. We agreed on a couple of things. Firstly we thought that language detection would not contribute a lot to our project. This is because the P2000 system and the local news sites are ensured to be in Dutch and it is very likely that the greater part of the Twitter data is also in Dutch. Next to that we also did not know how hard it was to detect a language, that is why we decided to give this detection a low priority and only try to implement it when we finished the rest. \\
We also concluded that it was enough to tokenize and stem the twitter/news data. This would improve the text significantly, because words who are very similar to each other such as werken, werkt and werk (which are Dutch words for working). All these words have the same stem. I have also searched for possibilities to fix real spelling mistakes such as aple instead of apple, but I found that there are certain tools who can do this for a small amount of input (such as a query), but not for a whole text file. So the focus was on tokenization and stemming.\\
After a bit of research I found a whole bunch of tools who supported either tokenization or stemming, but unfortunately not both. Happily we also found one tool which supported both. This tool is called Elasticsearch TODO REFERENCE. This is a tool where it is possible to store data on our own server (news and twitter data) and edit this data by applying several "analyzers" on them. These analyzers worked in a way such that you could add filters, like stemmers, and tokenizers to an analyzer which is saved. After you created the analyzer you could do a call to this analyzer and give it text as input. So the goal was now to create the analyzer we wanted and to make sure that it runs on our server. \\
I immediately noticed that there were a lot of options in Elasticsearch to modify text, but we were very limited due to the constraint that our text was in Dutch. There were in fact two good options for stemming and only one option as a tokenizer. As tokenizer we used a simple white space tokenizer. I tested a bunch of other tokenizers which were available but they did not give significant better results than the white space tokenizer and in combination with how easy to use the normal tokenizer is, we picked that one (punctuations were handled by the stemmers). \\
As a stemmer we had the possibility between a Hunspell stemmer, which uses a dictionary, and the Porter Snowball stemmer, which was based on an algorithm. After some searching we found a Dutch dictionary for the first stemmer, but we still decided to create the Snowball stemmer, because its results were better and we were not dependent on the quality of the dictionary. As a backup we also created a Hunspell analyzer. Both the analyzers can be found in TODO REFERENCE TOEVOEGEN VOOR ANALZYERS.\\
I have also thaught about adding phoenetic search, since a lot of the spelling errors in Dutch are mixing up whether a word should end with d or t or dt or use au instead of ou or ei instead of ij. All these variations do sound the same however, so it makes sense to use phoenetic search. Only the problem was almost all phoenetic search algorithms support english and we needed support for Dutch. I was not able to find an algorithm which supports Dutch, so unfortunately we were not able to make use of this type of search.

\subsubsection*{Evaluation $\&$ Possible Improvements }
The evaluation of the language detector can be done manually, since we can immediately see if that worked, if a lot of the words are corrected we might have detected the wrong language. To evaluate the spelling check is bit more tricky I guess. One thing that comes to my mind is to first index the retrieved data without a spelling check and after that do the same with a spelling check. This will be done using small text documents like news feed. Using this small documents we can manually check if the spelling corrector, did his work.
TODO: BREIDT DIT UIT EN IMPROVE DIT NA TESTEN
