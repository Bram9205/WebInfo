\subsection{Collect P2000 notifications}
\textbf{Name:} Francis Hoogendijk \indent \textbf{StudentNumber:} 0834628

\subsubsection*{Motivation}
As described in the introduction, the program should show events which required emergency services in a specified neighbourhood. For this we plan to use notifications of the Dutch P2000 system. These notifications can be accessed through several different websites and contain timestamped information about which emergency response team is required, where they should go and sometimes other information. Since the P2000 system is continually operational, the system should frequently check for new notifications.

\subsubsection*{Approach}
To obtain the notifications, a purpose-built website scraper was made for the website \url{http://www.p2000-online.net}. This particular website was chosen because it offers the notifications in a somewhat structured manner (table rows), which can be extracted using a simple DOM-parser.\\

The scraper was written in php, since the indexing of the P2000 notifications was also written in this language. This enabled easy interfacing between the two parts, and allowed for simple database access. \\

First, the chosen website was manually inspected to extract the relevant DOM-elements. The layout of these elements was hard-coded into the scraper for simplicity. The scraper loads a page, and extracts the relevant table containing the notifications. It then goes through the table, bundling together table rows that are relevant to a single notification. This number may vary due to capcodes, basically destination addresses for the emergency service's pagers. The notifications are stored as an array of strings containing the raw HTML code. At the end of each page, the array is passed on to the indexer, and the scraper continues on to the next page by using the information in the form of the button. For politeness, the scraper waits at least half a second (arbitrarily chosen) between consecutive page requests.\\

The indexer extracts the date, time, type (Ambulance, Fireman or Police), region, content and capcodes from the raw HTML string and afterwards stores those values in a database with a unique id. To make sure the database is up-to-date, this stored information was used by the scraper as feedback. It is set up to keep scraping pages until a notification is stored in the database of a given date. For this date, 14 days ago was arbitrarily chosen. An initial execution of the scraper will take the longest to fill the database. Any consecutive invocations will have the same target (14 days ago) but for efficiency will abort if it encounters the already stored notifications. To determine that it has reached this point, it receives feedback from the indexer if the particular notification has been indexed or not. If all notifications of a page have already been indexed, a counter is increased. If this counter reaches a yet again arbitrarily chosen value of 5, it stops scraping. This value was chosen greater than 1 since the website continually updates, and all notifications occurring in all of the Netherlands are considered. It frequently happens that when the scraper has gone over a page, that on the next page the same notifications are viewed again. This happens because the first page is updated with fresh notifications and thus all notifications are pushed through.\\

The scraper has been used to fill the database by running it manually invoked on a local machine. It then sent all information to the database. The scraper should be implemented on the same server as the database for more efficient scraping, combined with automated frequent invocations.

\subsubsection*{Evaluation $\&$ Possible Improvements}
The scraper was manually evaluated on performance and correctness by running it locally and outputting messages to the command line interface. Also when the indexer was made sure to function properly, the performance of the scraper could be checked by viewing the database contents (for this we used phpmyadmin). The data resembled the content of the website and the scraper was deemed to function properly.\\

The scraper at first needed to be slowed down for politeness, where the assumption of half a second between consecutive page loads appeared to be polite enough (the scraper was never blocked). After more functionality was added to the indexing and classification stage, the processing time per page increased beyond the half second bound, making the scraper even more polite, but requiring more time to scrape content to fill the database. \\
The person responsible for the indexing admitted it could be made more efficient but we agreed that for a proof of concept this approach suffices.\\

An improvement that may be made to is to incorporate another scraper for another website that shows P2000 notifications. Processing the contents and comparing them may increase the number of notifications that can be retrieved. This is because these websites are not official, and use decoders to detect notification messages. Also, some existing P2000 websites already show possible associations or geolocations. These may be used for reference but for the purpose of this course, extracting this information from those websites was deemed too complex.
